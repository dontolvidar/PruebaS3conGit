#!/usr/bin/env python
# coding: utf-8

# (1:prob4)=
# # <span style="color:#F72585">Distribuciones de probabilidad continuas</span>

# ## <span style="color:#4361EE">Introducci贸n</span>

# En esta lecci贸n se introducen las distribuciones de probabilidad continuas. Se introducen los conceptos de informaci贸n en distribuciones continuas y la entrop铆a diferencial.
# 
# Al empezar se introduce el concepto de variable aleatoria continua y la distribuci贸n normal. Si no recuerda el concepto de integral de una funci贸n en un intervalo, puede imaginar que una integral es aproximadamente un promedio de los valores de la funci贸n en un determinado intervalo.
# 
# Por ejemplo si una funci贸n $h(x)$ est谩 definida en un intervalo digamos $[a,b]$, entonces la integral de $h$ en ese intervalo es aproximadamente una suma pesada:
# 
# $$
# \int_{a}^{b} h(x)dx \approx p_1f(x_1) + p_2f(x_2) + \ldots + p_nf(x_n),
# $$
# 
# para algunos pesos $p_1,\ldots,p_n$ y algunos valores $x_1,\ldots,x_n$.
# 
# Esto es todo lo necesitamos saber para seguir la lecci贸n. Calcular una integral puede ser muy complicado, pero en esta lecci贸n solamente requerimos tener presente la ecuaci贸n anterior.

# (prob4:ej1)=
#  ## <span style="color:#4361EE">Variables aleatorias continuas</span>
# 
# 
# 
# 

# Supongamos que se mide con una herramienta de precisi贸n el di谩metro de la cabeza de un tornillo salido de un proceso de producci贸n. Es conocido que cada tornillo producido puede tener peque帽as diferencias de tama帽o. Esto se debe en general a la forma como cada tornillo es procesado. Las m谩quinas que los producen por lo general pierden precisi贸n por el uso. 
# 
# Otro ejemplo menos preciso pero m谩s cercano a nosotros es medir la estatura de las personas. En este caso los instrumentos son mucho menos precisos.

# 
# Si se define $X$ como la medida en cada caso, es claro que $X$ es una variable  aleatoria, puesto que al realizar los experimentos (en este caso las mediciones), no se puede predecir con total precisi贸n, cual ser谩 el valor medido en cada experimento. 
# 
# Por otro lado $X$ tiene una caracter铆stica diferente a las variables aleatorias discretas estudiadas con anterioridad. Sus posibles valores ahora no puede enumerarse. Dentro de un rango razonable es posible obtener como resultado cualquier n煤mero real dentro de ese rango. En este caso  se dice que la variable aleatoria es continua.
# 
# Para las variables aleatorias continuas no existe funci贸n de probabilidad, debido a que ahora para cada valor posible no es posible asociar una probabilidad diferente de cero. En este caso se toman intervalos tan grandes o peque帽os como sea posible y a estos intervalos se les asigna una medida de probabilidad.

#  ## <span style="color:#4361EE">Funci贸n de densidad de probabilidad</span>
# 

# La funci贸n de densidad de probabilidad, en adelante funci贸n de densidad, es el hom贸logo de la funci贸n de probabilidad. Se trata entonces de una funci贸n integrable no negativa, en donde el 谩rea bajo la curva es 1.
# 
# El ejemplo m谩s simple es la distribuci贸n uniforme en el intervalos $[0,1]$. En este caso la funci贸n de densidad es $g(x)=1$. Note que en este caso se tiene que
# 
# $$
# \int_0^1 g(x)dx = \int_0^1 1 dx= x|_{x=0}^{x=1} = 1-0 =1.
# $$
# 
# Esta distribuci贸n se llama uniforme porque cada subintervalo de $[0,1]$ tiene exactamente la misma probabilidad. Por ejemplo los intervalos $[0.1,0.3]$ y $[0.5,0.7]$ tiene cada uno medida de probabilidad 0.2. En este caso, la medida de probabilidad coincide con la medida de longitud que usamos a diario para medir longitudes, como por ejemplo, el frente de una casa.

# 
# ## <span style="color:#4361EE">Esperanza matem谩tica y varianza</span>
# 

# Las definiciones en este caso son generalizaciones a las estudiadas para el caso discreto, cambiado los promedios por integrales. Entonces se tiene que si $X$ es una variable aleatoria continua con densidad dada por $g(x)$, se tiene que la esperanza matem谩tica (o valor esperado) y la varianza son dadas por
# 
# $$
# \begin{align}
# \mathbb{E}[X] &= \mu_X=\int xg(x)dx\\
# \text{Var}[X] &= \int( x - \mu_X)^2g(x)dx\\
# \end{align}
# $$
# 
# Los l铆mites de las integrales dependen del tipo de distribuci贸n. De acuerdo con lo mencionado en la introducci贸n, en relaci贸n con el concepto de integral, podemos conservar las interpretaciones de la media y la varianza del caso discreto.
# 
# La media es aproximadamente el valor promedio de la variable aleatoria que se obtiene si se realiza muchas veces el experimento aleatorio y se calcula la medida dada por la variable aleatoria.
# 
# Por otro lado la varianza mide el grado de predictibilidad de la variable aleatoria. Entonces, aunque no son conceptos ex谩ctamente equivalentes podemos mantener las interpretaciones. Lo mismo ocurre con los conceptos de la teor铆a de informaci贸n.

# ### <span style="color:#4CC9F0">Ejercicio</span>

# Verifique la afirmaci贸n anterior. 
# 
# *Ayuda*: $\text{Prob}[0.5,0.7] =\int_{0.5}^{0.7} g(x)dx $.

# #### <span style="color:green">Escriba aqu铆 su respuesta. Discuta con sus compa帽eros.</span>

# (prob4:ej3)=
#  ## <span style="color:#4361EE">Distribuci贸n normal</span>

# La distribuci贸n normal es la distribuci贸n m谩s importante de toda la estad铆stica. Esto no solamente se debe  a su aplicaci贸n en gran cantidad de problemas, sino porque es la distribuci贸n l铆mite de muchas sucesiones de distribuciones que aparecen con frecuencia.  No entraremos en detalles t茅cnicos. Solamente vamos a indicar la expresi贸n de la funci贸n de densidad y veremos como calcular algunas probabilidades.
# 
# La densidad de la distribuci贸n normal tiene la expresi贸n
# 
# $$
# g(x) = \sqrt{\tfrac{1}{2\pi\sigma^2}} e^{-\tfrac{(x-\mu)^2}{2\sigma^2}}
# $$

# La expresi贸n es un poco intimidante, pero su uso con la ayuda de Python muy f谩cil. Vamos primero dar las expresiones para la media y la varianza dela distribuci贸n normal
# 
# 1. Los par谩metros de la distribuci贸n  normal son $\mu$ y $\sigma^2$.
# 2. La funci贸n $g(x)$ es sim茅trica alrededor de $\mu_X$.
# 
# Adem谩s si $X$ es una variable aleatoria con distribuci贸n normal, que escribiremos $X\sim \mathcal{N}(\mu,\sigma^2)$, entonces
# 
# 3. $\mathbb{E}[X] = \mu$
# 4. $\text{Var}[X] = \sigma^2$
# 
# El siguiente es un gr谩fico de la densidad de una distribuci贸n normal. Por favor revise el c贸digo Python cuidadosamente y haga cambios en los par谩metros para ver el efecto en la densidad.

# In[1]:


get_ipython().system('pip install matplotlib numpy scipy')


# In[2]:



import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats
import math
from matplotlib.widgets import Slider, Button

get_ipython().run_line_magic('matplotlib', 'inline')
mu = 0
variance = 1
sigma = math.sqrt(variance)
t = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)
#Fijar los limites de los ejes
plt.xlim(mu - 3*sigma, mu + 3*sigma)

plt.plot(t, stats.norm.pdf(t, mu, sigma))


# Para modificar mu y sigma por medio de deslizadores abra este cuaderno en Colab o Binder

# In[3]:


get_ipython().run_line_magic('matplotlib', 'notebook')
# Crear la figura y el line que manipularemos
fig, ax = plt.subplots()
line, = plt.plot(t, stats.norm.pdf(t, mu, sigma), lw=2)

# Ajustar el grafico para dejar espacio para los deslizadores
plt.subplots_adjust(left=0.25, bottom=0.25)

# Hacer un deslizador horizontal para el valor de mu.
axfreq = plt.axes([0.25, 0.1, 0.65, 0.03])
mu_slider = Slider(
    ax=axfreq,
    label='Mu',
    valmin=-3,
    valmax=3,
    valinit=mu,
)

# Hacer un deslizador vertical para el valor de sigma
axamp = plt.axes([0.1, 0.25, 0.0225, 0.63])
sigma_slider = Slider(
    ax=axamp,
    label="Sigma",
    valmin=0,
    valmax=5,
    valinit=sigma,
    orientation="vertical"
)

# La funcion que se llama cada vez que el valor en los deslizadores cambia
def update(val):
    line.set_ydata(stats.norm.pdf(t, mu_slider.val, sigma_slider.val))
    fig.canvas.draw_idle()

#Registro de la funcion update para cada deslizador
mu_slider.on_changed(update)
sigma_slider.on_changed(update)

#Creaci贸n Boton Reset
resetax = plt.axes([0.8, 0.025, 0.1, 0.04])
button = Button(resetax, 'Reset', hovercolor='0.975')


def reset(event):
    mu_slider.reset()
    sigma_slider.reset()
button.on_clicked(reset)

plt.show()


# In[4]:


# Prob[,] .
import scipy.stats as stats
import math
a = 8
mu = 10
var = 1
sigma = math.sqrt(var)
prob_inf_a = stats.norm.cdf(a, mu, sigma)
prob_inf_a


# ## <span style="color:#4361EE">Funci贸n de distribuci贸n acumulada</span>
# 

# Dada una distribuci贸n con densidad $g(x)$, su funci贸n de distribuci贸n acumulada evaluada en $x$ se define mediante la funci贸n $G$ dada por
# 
# $$
# G(x) = \int_{-\infty}^{x}g(x)dx
# $$
# 
# Observe que en consecuencia si $G$ es una funci贸n de distribuci贸n acumulada se tiene que para cualquier intervalo $[a,b]$
# 
# $$
# \text{Prob}[a,b] = G(b) - G(a) = \int_{-\infty}^{b}g(x)dx - \int_{-\infty}^{a}g(x)dx = \text{Prob}[-\infty,b] - \text{Prob}[-\infty,a].
# $$

# (prob4:ej4)=
# ## <span style="color:#4361EE">Informaci贸n de distribuciones continuas</span>
# 

# La siguiente gr谩fica muestra las funciones de informaci贸n (diferencial), definida por $-\log g(x)$. La informaci贸n diferencial representa el **grado de sorpresa** de observar el valor en cada punto.
# 
# 
# Tanto la distribuci贸n de Student(0,1,df=10) como la doble exponencial(0,1) tienen valores sorpresa muy por debajo de lo normal(0,1) en los rangos (-6, 6). Esto significa que los valores at铆picos tendr谩n menos efecto en el log-posterior de los modelos que usan estas distribuciones. Una l铆nea de regresi贸n necesitar铆a moverse menos para incorporar esas observaciones, ya que la distribuci贸n del error no las considerar谩 inusuales.

# In[5]:


import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm
from scipy.stats import t
from scipy.stats import laplace
#
fig, ax = plt.subplots(1, 1,figsize=(12, 8))
plt.title('Funciones de informaci贸n',fontsize=14)

x = np.linspace(-6,6,300)
#
y_laplace = -np.log(laplace.pdf(x))
ax.plot(x, y_laplace, 'g-', lw=2, alpha=0.6, label='laplace pdf')
#
y_norm = -np.log(norm.pdf(x))
ax.plot(x, y_norm, 'r-', lw=2, alpha=0.6, label='norm pdf')
#
y_student_t = -np.log(t.pdf(x, df=10))
ax.plot(x, y_student_t, 'b-', lw=2, alpha=0.6, label='student-t pdf')

#
plt.xlabel('$x$',fontsize=14)
plt.ylabel('$I(x) = -\log \ f(x)$',fontsize=14)
plt.legend()
plt.show()


# ## <span style="color:#4361EE">Entrop铆a diferencial</span>
# 

# 
# La entrop铆a de Shannon est谩 restringida a variables aleatorias que toman valores discretos. La f贸rmula correspondiente para una variable aleatoria continua con funci贸n de densidad de probabilidad $ f(x) $ con soporte (dominio) finito o infinito $ \mathbb{X}$ en la l铆nea real se define por analog铆a, utilizando la forma anterior de la entrop铆a como una esperanza:
# 
# $$h[f]= E [-\ln(f(x))]=-\int_{\mathbb {X}}f(x)\ln(f(x))dx.$$
# 
# 
# Esta f贸rmula generalmente se conoce como entrop铆a continua, o **entrop铆a diferencial**.
# 
# Aunque la analog铆a entre ambas funciones es sugestiva, debe establecerse la siguiente pregunta: 
# 
# `驴Es la entrop铆a diferencial una extensi贸n v谩lida de la entrop铆a discreta de Shannon?` 
# 
# 
# La entrop铆a diferencial carece de una serie de propiedades que la entrop铆a discreta de Shannon tiene, *incluso puede ser negativa*.
# 
# - $ \leadsto $ Se puede demostrar que la entrop铆a diferencial no es un l铆mite de la entrop铆a de Shannon para $ n \to \infty $.
#    
# Por otro lado, los conceptos y definiciones de la entrop铆a de Shanon se traducen con la misma interpretaci贸n al caso continuo. Las sumas ahora son integrales. As铆 tenemos  en el caso continuo que:
# 
# 1. Entrop铆a conjunta: $H(X,Y) = -\int \int f(x,y)\ln f(x,y) dx dy$.
# 2. Entrop铆a condicional: $H(Y|X)= -\int\int  f(x,y)\ln p(y|x) dx dy$.
# 3. Informaci贸n mutua: $\mathfrak{M}(X,Y)  = \int \int f(x,y)[\ln f(x,y) - \ln f_X(x)f_Y(y)]dx dy$.
# 

# ## <span style="color:#4361EE">Entrop铆a de la familia normal</span>
# 

# Se puede verificar que la entrop铆a diferencial de la familia de distribuciones normales est谩 dada por
# 
# $$
# Entropia_{normal} = \tfrac{1}{2} \ln(2\pi e\sigma^2 )
# $$
# 
# So observa entonces que tanto la varianza $\sigma^2$ como la entrop铆a $\tfrac{1}{2} \ln(2\pi e\sigma^2) $ son funciones crecientes. A mayor valor de $\sigma$ mayor entrop铆a y viceversa. As铆 una variable aleatoria que tiene distribuci贸n normal es mas predecible entre menor es su varianza.
# 
# La figura muestra la entrop铆a y la varianza en funci贸n de $\sigma$ para la familia normal.

# In[6]:


import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm
from scipy.stats import t
from scipy.stats import laplace
#
fig, ax = plt.subplots(1, 1,figsize=(12, 8))
plt.title('Entrop铆a y varianza en la familia normal como funci贸n de $\sigma$',fontsize=14)

sigma = np.linspace(0.4,2,300)
#
varianza = sigma**2
ax.plot(sigma, varianza, 'g-', lw=2, alpha=0.6, label='varianza')
#
entropia = 0.5*np.log(2*np.pi*np.e*sigma**2)
ax.plot(sigma, entropia, 'r-', lw=2, alpha=0.6, label='entrop铆a')
#
plt.xlabel('$\sigma$',fontsize=14)
plt.ylabel('entrop铆a =$\sqrt{\log (2\pi e \sigma^2) }$',fontsize=14)
plt.legend()
plt.show()


# ## <span style="color:#4361EE">Autores</span>

# 1. Alvaro Mauricio Montenegro D铆az, ammontenegrod@unal.edu.co
# 1. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com 

# ## <span style="color:#4361EE">Comentarios</span>
# 
