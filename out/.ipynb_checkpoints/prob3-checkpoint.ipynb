{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#F72585\">Probabilidad Conjunta y Entropía Cruzada</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Introducción</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta lección se introducen los concepto de función de probabilidad conjunta. Se introducen los conceptos de correlación,  información mutua y entropía cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">El ejemplo de la moneda cargada</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos el ejemplo de la Variable Bernoulli de la [lección de variables aleatorias](Prob_Variables_Aleatorias.ipynb). En realidad el ejemplo puede reescribirse usando como experimento el lanzamiento de una moneda cargada. Suponemos dos posibles resultados cara $(g=1)$ y sello $g=0$. Suponemos además que $\\text{Prob}[g=1]= 0.6$ y por tanto $\\text{Prob}[g=0]= 0.4$. El experimento consiste en lanzar tres veces la moneda cargada y anotar el resultado: cara (1) o sello (0).\n",
    "\n",
    "Ahora definimos dos variables aleatorias. La primera  que llamaremos $X$, corresponde a contar el número de caras que salen. De acuerdo con los resultados de la [lección de variables aleatorias](Prob_Variables_Aleatorias.ipynb), la función de probabilidad de $X$, denotada $p_X$ es dada por extensión de  la siguiente forma:\n",
    "\n",
    "|Valor |Experimentos| probabilidad cada experimento| probabilidad para este valor de $X$| total|\n",
    "|---|---|---| ---|---|\n",
    "|0| 000| $0.4\\times 0.4 \\times 0.4$|0.064|0.064|\n",
    "|1| 100| $0.6\\times 0.4 \\times 0.4$|0.096||\n",
    "|1| 010| $0.4 \\times 0.6\\times 0.4$|0.096||\n",
    "|1| 001| $0.4 \\times 0.4\\times 0.6$|0.096|0.288|\n",
    "|2| 110| $0.6\\times 0.6 \\times 0.4$|0.144||\n",
    "|2| 011| $0.4 \\times 0.6\\times 0.6$|0.144||\n",
    "|2| 101| $0.6 \\times 0.4 \\times 0.6$|0.144|0.432|\n",
    "|3| 111| $0.6 \\times 0.6\\times 0.6$|0.216|0.216 |\n",
    "\n",
    "```{admonition} Nota\n",
    "Recuerde que obtuvimos que $\\mathbb{E}[X]=1.8$.  \n",
    "\n",
    "Además se tiene que $\\text{Var}[X]=0.72$ y $\\sigma_X = 0.849$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejercicio</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por favor verifique que si $X\\sim \\text{Binom}(N,\\pi)$, entonces \n",
    "\n",
    "1. $\\mathbb{E}[X]=N\\pi$ \n",
    "2. $\\text{Var}[X]=N\\pi(1-\\pi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La segunda variable que llamaremos $Y$ se defien como sigue:\n",
    "\n",
    "$$\n",
    "Y = \\begin{cases}  0, &\\text{ si no sale ninguna cara}, \\\\\n",
    "1, &\\text{ si salen una o dos caras},\\\\\n",
    "-1, &\\text{ si salen tres caras}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "a función de probabilidad de  Y , denotada  $p_Y$  es dada por extensión de la siguiente forma:\n",
    "\n",
    "|Valor |Experimentos| probabilidad cada experimento| probabilidad para este valor de $Y$| total|\n",
    "|---|---|---| ---|---|\n",
    "|0| 000| $0.4\\times 0.4 \\times 0.4$|0.064|0.064|\n",
    "|1| 100| $0.6\\times 0.4 \\times 0.4$|0.096||\n",
    "|1| 010| $0.4 \\times 0.6\\times 0.4$|0.096||\n",
    "|1| 001| $0.4 \\times 0.4\\times 0.6$|0.096||\n",
    "|1| 110| $0.6\\times 0.6 \\times 0.4$|0.144||\n",
    "|1| 011| $0.4 \\times 0.6\\times 0.6$|0.144||\n",
    "|1| 101| $0.6 \\times 0.4 \\times 0.6$|0.144|0.720|\n",
    "|-1| 111| $0.6 \\times 0.6\\times 0.6$|0.216|0.216 |\n",
    "\n",
    "```{admonition} Recuerde que\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[Y] &= 0\\times 0.064 +  1\\times 0.72 +(-1)\\times 0.216 = 0.504\\\\\n",
    "\\text{Var}[Y] &=  (0-0.504)^2\\times 0.064 + (1-0.504)^2\\times 0.720 + (-1-0.504)^2\\times 0.216 = 0.682\\\\\n",
    "\\sigma_Y &= 0.826\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "```\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Función de probabilidad conjunta</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abordemos el problema de determinar como está relacionadas (asociadas) estas dos variables. Para empezar observe la siguiente tabla que muestra como coocurren los valores de las dos variables aleatorias\n",
    "\n",
    "\n",
    "|X/Y|0|1|-1|\n",
    "|---|---|---|---|\n",
    "|0|000|---------|---|\n",
    "|1|---|100 010 001|---|\n",
    "|2|---|101 110 011|---|\n",
    "|3|---|---------|111|\n",
    "\n",
    "La celdas vacias indican parejas de valores $(x,y)$ que no pueden ocurrir. La función de probabilidad conjunta de la variables $X$ y $Y$ se define como la función de dos variables dada por \n",
    "\n",
    "$$\n",
    "p_{XY}(x,y) = \\text{Prob}(X=x, Y=y).\n",
    "$$\n",
    "\n",
    "En nuestro ejemplo, la función de probabilidad conjunta es definida por extensión de la siguiente forma\n",
    "\n",
    "|X/Y|0|1|-1|$P_X$|\n",
    "|---|---|---|---|---|\n",
    "|0|0.064|0.000|0.000|0.064|\n",
    "|1|0.000|0.288|0.000|0.288|\n",
    "|2|0.000|0.432|0.000|0.432|\n",
    "|3|0.000|0.000| 0.216|0.216|\n",
    "|$P_Y$|0.064|0.720| 0.216|1.000|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Distribución marginal</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que en la última fila de la tabla se recupera la función de probabilidad de $Y$. De la misma forma, en la última columna  se recupera la función de probabilidad de $X$. En este contexto de funciones de probabilidad conjunta, las funciones $P_X$ y $P_Y$ se llaman funciones de probabilidad marginales. En este caso, cada valor corresponde a la suma de la fila o columna correspondiente. La celda inferior derecha muestra la suma total de probabilidades, que claro debe ser 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Correlación</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El concepto de correlación está completamente asociado a la funcion de distribución conjunta de dos variables aleatorias. Escencialmente, la correlación mide como covarían las dos variables aleatorias. Supongamos que $X$ y $Y$ son variables aleatorias definidas sobre el mismo espacio muestral como en el ejemplo anterior.\n",
    "\n",
    "Además supongamos que la media y la desviación estándar de cada variable se denotan como $\\mu_X$, $\\sigma_X$ y $\\mu_Y$ y $\\sigma_Y$ respectivamente. Entonces se tiene que la correlación entre las dos variables aleatorias es dada por\n",
    "\n",
    "$$\n",
    "Cor(X,Y) = \\mathbb{E}\\left[\\frac{(X-\\mu_X)}{\\sigma_X}\\frac{(Y-\\mu_Y)}{\\sigma_Y} \\right]= \\frac{\\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y)}{\\sigma_X \\sigma_Y}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a calcular la correlación de las variables del ejemplo anterior. Usaremos la segunda ecuación. La igualdad puede ser verificada sin mucha dificutad.\n",
    "\n",
    "La única cantidad que no tenemos aún es $\\mathbb{E}(XY)$, la cual puede calcularse como sigue:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(XY) = \\sum_i \\sum_j x_iy_j P_{XY}(x_i,y_j)\n",
    "$$\n",
    "\n",
    "En nuestro ejemplo tenemos que \n",
    "\n",
    "$$\n",
    "\\mathbb{E}(XY) =  1 \\times 1 \\times P_{XY}(1,1) + 2 \\times 1 \\times P_{XY}(2,1) + 3\\times (-1)\\times P_{XY}(3,-1) = 0.504\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En consecuencia se tiene que\n",
    "\n",
    "$$\n",
    "Cor(X,Y) = \\frac{0.504 - 1.8\\times 0.504}{0.849\\times 0.826} = -0.575\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Información mutua</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Este concepto es el análogo a la correlación, pero en este caso desde la\n",
    "teoría de la información de Shanon.\n",
    "\n",
    "Para dos distribuciones (variables aleatorias $X$ y $Y$) discretas conjuntamente distribuidas,\n",
    "la información conjunta se define por\n",
    "\n",
    "$$\n",
    "\\mathfrak{m}(X,Y)=\\sum_{y\\in Y}\\sum_{x\\in X}P_{XY}(x,y) \\log \\left(\\frac {P_{XY}(x,y)}{P_X(x)P_{Y}(y)}\\right) = \\mathbb{E}_{XY}[\\log P_{XY} - \\log P_X\\log P_Y]\n",
    "$$\n",
    "\n",
    "Observe que si las variables aleatorias son independientes, entonces $\\mathfrak{m}(X,Y)=0$, porque $\\log P_{XY} = \\log P_X\\log P_Y$.\n",
    "\n",
    "Por otro lado, Si $X=Y$, se tiene que $\\mathfrak{m}(X,X) =  H(X)$, es decir, la entropía de $X$.\n",
    "\n",
    "La información mutua se puede calcular siempre, teniendo en cuenta el convenio $0 \\log 0 = 0$.\n",
    "\n",
    "Recordemos la tabla de probabilidad conjunta del ejemplo:\n",
    "\n",
    "|X/Y|0|1|-1|$P_X$|\n",
    "|---|---|---|---|---|\n",
    "|0|0.064|0.000|0.000|0.064|\n",
    "|1|0.000|0.288|0.000|0.288|\n",
    "|2|0.000|0.432|0.000|0.432|\n",
    "|3|0.000|0.000| 0.216|0.216|\n",
    "|$P_Y$|0.064|0.720| 0.216|1.000|\n",
    "\n",
    "\n",
    "Entonces en  este caso tenemos que\n",
    "\n",
    "$$\\mathfrak{m}(X,Y) = 0.064 \\log\\tfrac{0.064}{0.064\\times 0.064} +0.288 \\log \\tfrac{ 0.288}{0.288\\times 0.724} +0.432\\log\\tfrac{0.432}{0.432\\times 0.72} + 0.216 \\log \\tfrac{0.216}{0.216*0.216} = 0.5032.\n",
    "$$\n",
    "\n",
    "La información mutua siempre es positiva e indica la cantidad de información que las dos variables cargan conjuntamente, la una de la otra.\n",
    "\n",
    "La siguiente línea ilustra el cálculo usando Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "(0.064)*np.log(0.064/(0.064*.064)) +(0.288)*np.log(0.288/(0.288*0.724)) \\\n",
    "+(0.432)*np.log(0.432/(0.432*0.72) + 0.216*np.log(0.216/(0.216*0.216)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Entropía cruzada</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En teoría de la información, la entropía cruzada entre dos distribuciones de probabilidad  $P$ y $Q$ sobre el mismo espacio muestral mide el número promedio de bits (o nats) necesarios para identificar una distribución con la otra. \n",
    "\n",
    "En la práctica, si se considera la distribución $P$ como la distribución verdadera y a $Q$ como una distribución aproximante, entonces la entropía cruzada se define mediante\n",
    "\n",
    "\n",
    "$$\n",
    "H(P,Q) = - E_P[ \\log Q]  = - (p_1\\log q_1 + p_2\\log q_2+ \\cdots + p_n\\log q_n)\n",
    "$$\n",
    "\n",
    "\n",
    "**Interpretación**\n",
    "\n",
    "Para la interpretación, esta es una medida de que tanto difiere $Q$ de $P$, medido en bits o nats. Entonces entre menor es este valor, mejor es la aproximación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ilustrar el concepto, calculemos la entropía cruzada entre las distribuciones binomiales $ P=Bin(3, 0.3)$, $Q_1=Bin(3, 0.4)$ y $Q_2=Bin(3,0.7)$.\n",
    "\n",
    "Vamos considerar a la distribución $P$ como la verdadera y a $Q_1$ y $Q_2$ como aproximantes. Usted debe sospechar que $Q_1$ es mejor aproximante que $Q_2$.\n",
    "\n",
    "Veámos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N, p, q1, q2  = 3, 0.3, 0.4, 0.7\n",
    "\n",
    "P = [binom.pmf(k,N,p) for k in range(N+1)]\n",
    "Q1 = [binom.pmf(k,N,q1) for k in range(N+1)]\n",
    "Q2 = [binom.pmf(k,N,q2) for k in range(N+1)]\n",
    "\n",
    "print(np.round(P,3))\n",
    "print(np.round(Q1,3))\n",
    "print(np.round(Q2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_P_Q1 = -np.sum(P*np.log(Q1))\n",
    "H_P_Q2 = -np.sum(P*np.log(Q2))\n",
    "\n",
    "print('H(P,Q1)= ', H_P_Q1 )\n",
    "print('H(P,Q2)= ', H_P_Q2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente un gráfico de las ttes distribuciones.\n",
    "\n",
    "Discuta los resultados en la clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax = axes.ravel()\n",
    "label = ['0','1','2','3']\n",
    "\n",
    "ax[0].bar(label, P, color = 'orange', edgecolor='blue',width=1)\n",
    "ax[0].set_title(\"Distribución P\")\n",
    "\n",
    "ax[1].bar(label, Q1, color = 'blue', edgecolor='red',width=1)\n",
    "ax[1].set_title(\"Distribución Q1\")\n",
    "\n",
    "ax[2].bar(label, Q2, color = 'red', edgecolor='blue',width=1)\n",
    "ax[2].set_title(\"Distribución Q2\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Entropía cruzada en aprendizaje de máquinas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que  que una red neuronal de clasificación transforma un tensores de entrada en  una distribuciones. Tal distribución indica las probsbilidades de que el tensor de entrada pertezca a cada una de las clases.\n",
    "\n",
    "En el entrenamiento se busca encontrar los pesos sinápticos que minimizan conjuntamente la entropía cruzada entre la distribución de salida de los tensores calculada por la red neuronal y la distribución verdadera asociada a cada tensor.\n",
    "\n",
    "Por ejemplo, supongamos que se tienen tres clases y que para un determinado tensor la clase asociada es la 1 (las posibles clases son 0,1,2).\n",
    "\n",
    "Entonces, la distribución verdadera en este caso es $P=(0,1,0)$. Por otro lado, supongamos que la distribución que calcula la red neuronal para este tensor es $Q= (0.2, 0.7, 0.1)$. Entonces, la entropía cruzada para este tensor es dada por\n",
    "\n",
    "$$\n",
    "H(P,Q) = - 1 \\log 0.7 = 0.357\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Comentarios</span>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
